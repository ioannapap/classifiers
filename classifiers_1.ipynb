{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import scipy as sp\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as  plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "####\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open training dataset and parse recipes\n",
    "\n",
    "with open('train.json') as cooking_file:  \n",
    "    \n",
    "    data = json.load(cooking_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTVECTORIZER\n",
    "#removing punctuations and spaces before fixing the data and keep them all in a dict  (italian, mexican)\n",
    "\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "def gather_recipes(recipes, cuisine_country):\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for _object in recipes:\n",
    "        \n",
    "        if _object.get('cuisine') == cuisine_country:\n",
    "            \n",
    "            ingredients = _object.get('ingredients') \n",
    "\n",
    "            _id = _object.get('id')\n",
    "            \n",
    "            for i in ingredients:\n",
    "                \n",
    "                _dict = dict()\n",
    "                ingr = i.replace(' ', '').lower()\n",
    "                    \n",
    "                for ch in ingr: \n",
    "                    \n",
    "                    if ch in punctuations: \n",
    "                        \n",
    "                        ingr = ingr.replace(ch, '')  \n",
    "                \n",
    "                _dict['ingredients'] = ingr                \n",
    "                _dict['id'] = _id\n",
    "                data_list.append(_dict)\n",
    "                \n",
    "    return data_list\n",
    "\n",
    "\n",
    "clean_italian_cuisine_data = gather_recipes(data, 'italian')\n",
    "clean_mexican_cuisine_data = gather_recipes(data, 'mexican')\n",
    "\n",
    "clean_filipino_cuisine_data = gather_recipies(data, 'filipino')\n",
    "clean_british_cuisine_data = gather_recipies(data, 'british')\n",
    "clean_moroccan_cuisine_data = gather_recipies(data, 'moroccan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dfs  (italian, mexican)\n",
    "\n",
    "italian_df = DataFrame(clean_italian_cuisine_data)\n",
    "italian_ingredients = list(set(italian_df.ingredients))\n",
    "\n",
    "mexican_df = DataFrame(clean_mexican_cuisine_data)\n",
    "mexican_ingredients = list(set(mexican_df.ingredients))\n",
    "\n",
    "\n",
    "#creating dfs (filipino, british, moroccan )\n",
    "\n",
    "filipino_df = DataFrame(clean_filipino_cuisine_data) \n",
    "filipino_ingredients = list(set(filipino_df.ingredients))\n",
    "\n",
    "british_df = DataFrame(clean_british_cuisine_data)\n",
    "british_ingredients = list(set(british_df.ingredients))\n",
    "\n",
    "moroccan_df = DataFrame(clean_moroccan_cuisine_data)\n",
    "moroccan_ingredients = list(set(moroccan_df.ingredients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing data for CountVectorizer (italian, mexican)\n",
    "\n",
    "it_unique_ids = []\n",
    "it_ingredients = []\n",
    "it_grouped = italian_df.groupby('id')\n",
    "\n",
    "for ids, ing in it_grouped:\n",
    "    \n",
    "    it_unique_ids.append(ids)\n",
    "    \n",
    "    row = str(ing)\n",
    "    row = row.replace('ingredients','')\n",
    "    row = row.replace('id', '')\n",
    "    row = row.replace('\\n', '')\n",
    "    row = re.sub('[0-9]+', '', row)\n",
    "    row = row.split( )\n",
    "    row = ' '.join(row)\n",
    "    \n",
    "    it_ingredients.append(row)\n",
    "\n",
    "    \n",
    "mex_unique_ids = []\n",
    "mex_ingredients = []\n",
    "mex_grouped = mexican_df.groupby('id')\n",
    "\n",
    "for ids, ing in mex_grouped:\n",
    "    \n",
    "    mex_unique_ids.append(ids)\n",
    "    \n",
    "    row = str(ing)\n",
    "    row = row.replace('ingredients','')\n",
    "    row = row.replace('id', '')\n",
    "    row = row.replace('\\n', '')\n",
    "    row = re.sub('[0-9]+', '', row)\n",
    "    row = row.split( )\n",
    "    row = ' '.join(row)\n",
    "    \n",
    "    mex_ingredients.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using CountVectorizer  (italian, mexican) and adding one column Cuisine\n",
    "\n",
    "it_vectorizer = CountVectorizer(analyzer = 'word', binary = True)\n",
    "it_array = it_vectorizer.fit_transform(it_ingredients)\n",
    "it_array = it_array.toarray()\n",
    "it_df = DataFrame(it_array, columns = it_vectorizer.get_feature_names(), index = it_unique_ids)\n",
    "it_df['Cuisine'] = 0 #italian\n",
    "it_df.to_csv('it_cleandata.csv', index_label = 'ID')\n",
    "#print(it_df)\n",
    "\n",
    "mex_vectorizer = CountVectorizer(analyzer = 'word', binary = True)\n",
    "mex_array = mex_vectorizer.fit_transform(mex_ingredients)\n",
    "mex_array = mex_array.toarray()\n",
    "mex_df = DataFrame(mex_array, columns = mex_vectorizer.get_feature_names(), index = mex_unique_ids)\n",
    "mex_df['Cuisine'] = 1 #mexican\n",
    "mex_df.to_csv('mex_cleandata.csv', index_label = 'ID')\n",
    "#print(mex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting mexican and italian in one DataFrame\n",
    "\n",
    "df = pd.DataFrame(it_df)\n",
    "df = df.append(mex_df, sort = False)\n",
    "df.fillna(0, inplace = True)\n",
    "df = df.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDFVECTORIZER\n",
    "#removing punctuations before fixing the data and keep them all in a dict  (italian, mexican)\n",
    "\n",
    "def gather_recipes(recipes, cuisine_country):\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for _object in recipes:\n",
    "        \n",
    "        if _object.get('cuisine') == cuisine_country:\n",
    "            \n",
    "            ingredients = _object.get('ingredients') \n",
    "            \n",
    "            _id = _object.get('id')\n",
    "            \n",
    "            for i in ingredients:\n",
    "                \n",
    "                _dict = dict()\n",
    "                ingr = i.lower()\n",
    "                    \n",
    "                for ch in ingr: \n",
    "                    \n",
    "                    if ch in punctuations: \n",
    "                        \n",
    "                        ingr = ingr.replace(ch, '')  \n",
    "                \n",
    "                _dict['ingredients'] = ingr                \n",
    "                _dict['id'] = _id\n",
    "                data_list.append(_dict)\n",
    "                \n",
    "    return data_list\n",
    "\n",
    "clean2_italian_cuisine_data = gather_recipes(data, 'italian')\n",
    "clean2_mexican_cuisine_data = gather_recipes(data, 'mexican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dfs2  (italian, mexican)\n",
    "\n",
    "italian2_df = DataFrame(clean2_italian_cuisine_data)\n",
    "mexican2_df = DataFrame(clean2_mexican_cuisine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing data for TfidfVectorizer (italian, mexican)\n",
    "\n",
    "it2_unique_ids = []\n",
    "it2_ingredients = []\n",
    "it2_grouped = italian2_df.groupby('id')\n",
    "\n",
    "for ids, ing in it2_grouped:\n",
    "    \n",
    "    it2_unique_ids.append(ids)\n",
    "    \n",
    "    row = str(ing)\n",
    "    row = row.replace('ingredients','')\n",
    "    row = row.replace('id', '')\n",
    "    row = row.replace('\\n', '')\n",
    "    row = re.sub('[0-9]+', '', row)\n",
    "    row = row.split( )\n",
    "    row = ' '.join(row)\n",
    "    it2_ingredients.append(row)\n",
    "\n",
    "    \n",
    "mex2_unique_ids = []\n",
    "mex2_ingredients = []\n",
    "mex2_grouped = mexican2_df.groupby('id')\n",
    "\n",
    "for ids, ing in mex2_grouped:\n",
    "    \n",
    "    mex2_unique_ids.append(ids)\n",
    "    \n",
    "    row = str(ing)\n",
    "    row = row.replace('ingredients','')\n",
    "    row = row.replace('id', '')\n",
    "    row = row.replace('\\n', '')\n",
    "    row = re.sub('[0-9]+', '', row)\n",
    "    row = row.split( )\n",
    "    row = ' '.join(row)\n",
    "    mex2_ingredients.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using TfidfVectorizer  (italian, mexican) and adding one column Cuisine\n",
    "\n",
    "it2_vectorizer = TfidfVectorizer(analyzer = 'word', binary = True)\n",
    "it2_array = it2_vectorizer.fit_transform(it2_ingredients)\n",
    "it2_array = it2_array.toarray()\n",
    "it2_df = DataFrame(it2_array, columns = it2_vectorizer.get_feature_names(), index = it2_unique_ids)\n",
    "it2_df['Cuisine'] = 0 #italian\n",
    "it2_df.to_csv('it2_cleandata.csv', index_label = 'ID')\n",
    "#print(it2_df)\n",
    "\n",
    "mex2_vectorizer = CountVectorizer(analyzer = 'word', binary = True)\n",
    "mex2_array = mex2_vectorizer.fit_transform(mex2_ingredients)\n",
    "mex2_array = mex2_array.toarray()\n",
    "mex2_df = DataFrame(mex2_array, columns = mex2_vectorizer.get_feature_names(), index = mex2_unique_ids)\n",
    "mex2_df['Cuisine'] = 1 #mexican\n",
    "mex2_df.to_csv('mex2_cleandata.csv', index_label = 'ID')\n",
    "#print(mex2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       abbamele   ac  accent  acini  acorn  active  added  adobo  agave  aged  \\\n",
      "4           0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "14          0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "20          0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "56          0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "62          0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "...         ...  ...     ...    ...    ...     ...    ...    ...    ...   ...   \n",
      "49678       0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "49687       0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "49707       0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "49709       0.0  0.0     0.0    0.0    0.0     0.0    0.0    0.0    0.0   0.0   \n",
      "49717       0.0  0.0     0.0    0.0    0.0     0.0    0.0    1.0    0.0   0.0   \n",
      "\n",
      "       ...  wensleydale  wesson  wheel  wing  wish  wolf  xanthan  yellowtail  \\\n",
      "4      ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "14     ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "20     ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "56     ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "62     ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "...    ...          ...     ...    ...   ...   ...   ...      ...         ...   \n",
      "49678  ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "49687  ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "49707  ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "49709  ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "49717  ...          0.0     0.0    0.0   0.0   0.0   0.0      0.0         0.0   \n",
      "\n",
      "       yuca  zero  \n",
      "4       0.0   0.0  \n",
      "14      0.0   0.0  \n",
      "20      0.0   0.0  \n",
      "56      0.0   0.0  \n",
      "62      0.0   0.0  \n",
      "...     ...   ...  \n",
      "49678   0.0   0.0  \n",
      "49687   0.0   0.0  \n",
      "49707   0.0   0.0  \n",
      "49709   0.0   0.0  \n",
      "49717   0.0   0.0  \n",
      "\n",
      "[14276 rows x 2007 columns]\n"
     ]
    }
   ],
   "source": [
    "#putting mexican and italian in one DataFrame\n",
    "\n",
    "df2 = pd.DataFrame(it2_df)\n",
    "df2 = df2.append(mex2_df, sort = False)\n",
    "df2.fillna(0, inplace = True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Logistic Regression \n",
    "lr_m = linear_model.LogisticRegression()\n",
    "\n",
    "# Support Vector Machines\n",
    "svm_m = svm.LinearSVC(random_state=0)\n",
    "#Decision Trees\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "\n",
    "#k-NN\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "# Naive Bayes\n",
    "gnb = BernoulliNB(binarize = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* CountVectorizer *********\n",
      "(14276, 3976)\n"
     ]
    }
   ],
   "source": [
    "print('********* CountVectorizer *********')\n",
    "X = df.loc[:, df.columns != 'Cuisine'].values  #all columns with values except Cuisine column\n",
    "print(X.shape)\n",
    "\n",
    "Y = df['Cuisine'] #just the ids in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Logistic Regression mean scores****\n",
      "scores: {'fit_time': array([2.10236812, 2.28280258, 2.22201824, 2.11605668, 2.0456109 ]), 'score_time': array([0.0265801 , 0.0260942 , 0.02617979, 0.02655578, 0.02633691]), 'test_accuracy': array([0.96813725, 0.97197898, 0.96882662, 0.96672504, 0.96882662]), 'test_precision_weighted': array([0.96837424, 0.9720495 , 0.96895965, 0.96685339, 0.96884661]), 'test_recall_weighted': array([0.96813725, 0.97197898, 0.96882662, 0.96672504, 0.96882662]), 'test_f1_weighted': array([0.96809087, 0.97195581, 0.96879183, 0.96668815, 0.96880957])}\n",
      "\n",
      "mean score test accuracy: 0.9688989045705847\n",
      "\n",
      "mean score test precision weighted: 0.9690166784300608\n",
      "\n",
      "mean score test recall weighted: 0.9688989045705847\n",
      "\n",
      "mean score test f1-measure weighted: 0.9688672451742961\n",
      "Confusion Matrix: [[7682  156]\n",
      " [ 288 6150]]\n"
     ]
    }
   ],
   "source": [
    "s = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "scores_ = model_selection.cross_validate(lr_m, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****Logistic Regression mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(lr_m, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****SVM mean scores****\n",
      "scores: {'fit_time': array([0.53595734, 0.50038743, 0.50931048, 0.48424864, 0.50178266]), 'score_time': array([0.02863026, 0.02821827, 0.02669287, 0.02987385, 0.02583671]), 'test_accuracy': array([0.96148459, 0.96567426, 0.96532399, 0.96707531, 0.96497373]), 'test_precision_weighted': array([0.96156998, 0.96567343, 0.96536147, 0.9671225 , 0.96509168]), 'test_recall_weighted': array([0.96148459, 0.96567426, 0.96532399, 0.96707531, 0.96497373]), 'test_f1_weighted': array([0.96144659, 0.96566187, 0.96529947, 0.96705088, 0.96499409])}\n",
      "\n",
      "mean score test accuracy: 0.9649063758603266\n",
      "\n",
      "mean score test precision weighted: 0.9649638117628687\n",
      "\n",
      "mean score test recall weighted: 0.9649063758603266\n",
      "\n",
      "mean score test f1-measure weighted: 0.96489058154031\n",
      "Confusion Matrix: [[7616  222]\n",
      " [ 279 6159]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(svm_m, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****SVM mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(svm_m, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****d-tree mean scores****\n",
      "scores: {'fit_time': array([15.86993647, 15.41277719, 15.96362233, 16.15348339, 10.87315965]), 'score_time': array([0.02547932, 0.02553463, 0.02614665, 0.0252068 , 0.02573919]), 'test_accuracy': array([0.94187675, 0.9502627 , 0.93625219, 0.9408056 , 0.9352014 ]), 'test_precision_weighted': array([0.94188568, 0.95026441, 0.93626193, 0.94079529, 0.9352959 ]), 'test_recall_weighted': array([0.94187675, 0.9502627 , 0.93625219, 0.9408056 , 0.9352014 ]), 'test_f1_weighted': array([0.94188075, 0.95023338, 0.9362566 , 0.9407994 , 0.93522913])}\n",
      "\n",
      "mean score test accuracy: 0.9408797284237689\n",
      "\n",
      "mean score test precision weighted: 0.9409006438491231\n",
      "\n",
      "mean score test recall weighted: 0.9408797284237689\n",
      "\n",
      "mean score test f1-measure weighted: 0.9408798521748334\n",
      "Confusion Matrix: [[7411  427]\n",
      " [ 422 6016]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(dtree, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****d-tree mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(dtree, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****k-NN mean scores****\n",
      "scores: {'fit_time': array([4.20346427, 4.18154025, 4.1372292 , 4.12692189, 4.13048267]), 'score_time': array([167.73319101, 169.05296135, 169.30660701, 167.97744846,\n",
      "       168.02565145]), 'test_accuracy': array([0.88935574, 0.90262697, 0.88581436, 0.89352014, 0.89492119]), 'test_precision_weighted': array([0.89416896, 0.90782103, 0.89262856, 0.90016256, 0.89938992]), 'test_recall_weighted': array([0.88935574, 0.90262697, 0.88581436, 0.89352014, 0.89492119]), 'test_f1_weighted': array([0.88823861, 0.90164085, 0.88436686, 0.89223242, 0.89392876])}\n",
      "\n",
      "mean score test accuracy: 0.8932476808586832\n",
      "\n",
      "mean score test precision weighted: 0.8988342046653408\n",
      "\n",
      "mean score test recall weighted: 0.8932476808586832\n",
      "\n",
      "mean score test f1-measure weighted: 0.8920815014094245\n",
      "Confusion Matrix: [[7539  299]\n",
      " [1225 5213]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(knn, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****k-NN mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(knn, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Naive-Bayes mean scores****\n",
      "scores: {'fit_time': array([0.48707676, 0.47063208, 0.47015071, 0.47038531, 0.48076653]), 'score_time': array([0.08609247, 0.02903891, 0.02929163, 0.0293169 , 0.02886987]), 'test_accuracy': array([0.96148459, 0.96952715, 0.96462347, 0.96217163, 0.96812609]), 'test_precision_weighted': array([0.96237002, 0.96983328, 0.96514423, 0.96265902, 0.96816809]), 'test_recall_weighted': array([0.96148459, 0.96952715, 0.96462347, 0.96217163, 0.96812609]), 'test_f1_weighted': array([0.96137117, 0.96947715, 0.96454557, 0.96209066, 0.96810371])}\n",
      "\n",
      "mean score test accuracy: 0.9651865860179448\n",
      "\n",
      "mean score test precision weighted: 0.9656349263771837\n",
      "\n",
      "mean score test recall weighted: 0.9651865860179448\n",
      "\n",
      "mean score test f1-measure weighted: 0.9651176533708903\n",
      "Confusion Matrix: [[7706  132]\n",
      " [ 365 6073]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(gnb, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****Naive-Bayes mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(gnb, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* TfidfVectorizer *********\n",
      "(14276, 2006)\n"
     ]
    }
   ],
   "source": [
    "print('********* TfidfVectorizer *********')\n",
    "X = df2.loc[:, df2.columns != 'Cuisine'].values  #all columns with values except Cuisine column\n",
    "print(X.shape)\n",
    "\n",
    "Y = df2['Cuisine'] #just the ids in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Logistic Regression mean scores****\n",
      "scores: {'fit_time': array([0.49483085, 0.48494387, 0.50219655, 0.49449825, 0.47824216]), 'score_time': array([0.00907183, 0.0092442 , 0.00911999, 0.01040626, 0.00946355]), 'test_accuracy': array([0.99264706, 0.99334501, 0.99299475, 0.99544658, 0.99404553]), 'test_precision_weighted': array([0.99274423, 0.99342468, 0.99308297, 0.99548405, 0.99410944]), 'test_recall_weighted': array([0.99264706, 0.99334501, 0.99299475, 0.99544658, 0.99404553]), 'test_f1_weighted': array([0.9926413 , 0.9933403 , 0.99298951, 0.99544444, 0.99404182])}\n",
      "\n",
      "mean score test accuracy: 0.9936957865457916\n",
      "\n",
      "mean score test precision weighted: 0.9937690761971514\n",
      "\n",
      "mean score test recall weighted: 0.9936957865457916\n",
      "\n",
      "mean score test f1-measure weighted: 0.993691474802611\n",
      "Confusion Matrix: [[7838    0]\n",
      " [  90 6348]]\n"
     ]
    }
   ],
   "source": [
    "s = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "scores_ = model_selection.cross_validate(lr_m, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****Logistic Regression mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(lr_m, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****SVM mean scores****\n",
      "scores: {'fit_time': array([0.19914865, 0.19150639, 0.19096613, 0.19335699, 0.19472957]), 'score_time': array([0.00932002, 0.0097084 , 0.00915241, 0.00927067, 0.00955343]), 'test_accuracy': array([0.99684874, 0.99544658, 0.99544658, 0.99579685, 0.99579685]), 'test_precision_weighted': array([0.99686672, 0.99547285, 0.99548403, 0.99582879, 0.99582879]), 'test_recall_weighted': array([0.99684874, 0.99544658, 0.99544658, 0.99579685, 0.99579685]), 'test_f1_weighted': array([0.99684772, 0.99544477, 0.99544443, 0.99579503, 0.99579503])}\n",
      "\n",
      "mean score test accuracy: 0.995867120928932\n",
      "\n",
      "mean score test precision weighted: 0.9958962354773305\n",
      "\n",
      "mean score test recall weighted: 0.995867120928932\n",
      "\n",
      "mean score test f1-measure weighted: 0.9958653957896434\n",
      "Confusion Matrix: [[7837    1]\n",
      " [  58 6380]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(svm_m, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****SVM mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(svm_m, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****d-tree mean scores****\n",
      "scores: {'fit_time': array([2.89653134, 2.98665929, 2.93397117, 2.79936361, 3.13967037]), 'score_time': array([0.01443267, 0.01437497, 0.01480556, 0.01532197, 0.01470089]), 'test_accuracy': array([0.99754902, 0.99789842, 0.99789842, 0.99789842, 0.99929947]), 'test_precision_weighted': array([0.99755991, 0.99790179, 0.99790179, 0.99790179, 0.99930037]), 'test_recall_weighted': array([0.99754902, 0.99789842, 0.99789842, 0.99789842, 0.99929947]), 'test_f1_weighted': array([0.99754841, 0.99789813, 0.99789813, 0.99789813, 0.99929943])}\n",
      "\n",
      "mean score test accuracy: 0.9981087531334776\n",
      "\n",
      "mean score test precision weighted: 0.9981131296658168\n",
      "\n",
      "mean score test recall weighted: 0.9981087531334776\n",
      "\n",
      "mean score test f1-measure weighted: 0.9981084437066656\n",
      "Confusion Matrix: [[7837    1]\n",
      " [  27 6411]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(dtree, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****d-tree mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(dtree, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****k-NN mean scores****\n",
      "scores: {'fit_time': array([2.31949806, 2.30487847, 2.29804659, 2.35382581, 2.30064058]), 'score_time': array([80.37389684, 80.13014698, 80.33800316, 80.10853553, 81.24009514]), 'test_accuracy': array([0.92927171, 0.93625219, 0.93485114, 0.93625219, 0.93730298]), 'test_precision_weighted': array([0.93734352, 0.94288196, 0.94175974, 0.94288575, 0.94373068]), 'test_recall_weighted': array([0.92927171, 0.93625219, 0.93485114, 0.93625219, 0.93730298]), 'test_f1_weighted': array([0.9284027 , 0.93557536, 0.93413779, 0.93557844, 0.93665572])}\n",
      "\n",
      "mean score test accuracy: 0.9347860405107753\n",
      "\n",
      "mean score test precision weighted: 0.9417203315935836\n",
      "\n",
      "mean score test recall weighted: 0.9347860405107753\n",
      "\n",
      "mean score test f1-measure weighted: 0.934070001235187\n",
      "Confusion Matrix: [[7838    0]\n",
      " [ 931 5507]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(knn, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****k-NN mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(knn, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Naive-Bayes mean scores****\n",
      "scores: {'fit_time': array([0.11876631, 0.09500575, 0.09351659, 0.09325671, 0.09411526]), 'score_time': array([0.01413131, 0.0099349 , 0.01023459, 0.0100646 , 0.01016808]), 'test_accuracy': array([0.97268908, 0.97863398, 0.97513135, 0.97443082, 0.98143608]), 'test_precision_weighted': array([0.97398327, 0.97943405, 0.97620864, 0.97556896, 0.98204342]), 'test_recall_weighted': array([0.97268908, 0.97863398, 0.97513135, 0.97443082, 0.98143608]), 'test_f1_weighted': array([0.97259409, 0.97857849, 0.97505392, 0.97434897, 0.98139545])}\n",
      "\n",
      "mean score test accuracy: 0.9764642599596757\n",
      "\n",
      "mean score test precision weighted: 0.9774476686666732\n",
      "\n",
      "mean score test recall weighted: 0.9764642599596757\n",
      "\n",
      "mean score test f1-measure weighted: 0.9763941854374604\n",
      "Confusion Matrix: [[7838    0]\n",
      " [ 336 6102]]\n"
     ]
    }
   ],
   "source": [
    "scores_ = model_selection.cross_validate(gnb, X, Y, scoring = s, cv = 5)                                 \n",
    "print('****Naive-Bayes mean scores****')\n",
    "print('scores:', scores_)\n",
    "print('\\nmean score test accuracy:', scores_['test_accuracy'].mean())\n",
    "print('\\nmean score test precision weighted:', scores_['test_precision_weighted'].mean())\n",
    "print('\\nmean score test recall weighted:', scores_['test_recall_weighted'].mean())\n",
    "print('\\nmean score test f1-measure weighted:', scores_['test_f1_weighted'].mean())\n",
    "\n",
    "Y_pred = cross_val_predict(gnb, X, Y, cv = 5)\n",
    "conf_matrix = confusion_matrix(Y, Y_pred)\n",
    "print('Confusion Matrix:', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
